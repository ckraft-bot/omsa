---
title: "Homework_1"
author: "Claire Kraft"
date: "2024-08-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 2.1
**Describe a situation or problem from your job, everyday life, current events, etc., for which a classification model would be appropriate. List some (up to 5) predictors that you might use.**

- I got into rock climbing less than a year ago. I think a classification model would be very useful for detecting climbing routes. In climbing there are route setter who configure the climbing "problems" by strategically screwing various climbing holds on the wall. It'd be neat if a classification model can scan the walls and determine the climbing style by looking at both the climbing holds and configuration of the holds. Some climbs are set up to force more dynamic moves and other climbs (called "slabs") which force technical, precise, and static moves. The prediction model can label the climbing routes or problems as either a dynamic climb or slab climb. 

- When not climbing I enjoy speed cubing (solving Rubik’s cube quickly). I just started competing this past year. A classification model could be beneficial to my training session. The model can detect my turns per second and case recognition & prediction. For the classic 3x3 Rubik’s cube there are a few methods to getting the puzzle to a solved state. In this case we'll just consider the CFOP method which is: cross + first two layers + orientation of the last layer + permutation of the last layer. Simply put this method is much like baking a cake (layer by layer). As I'm solving i do not have to watch my hands as i manipulate the pieces instead i commit almost everything to muscle memory and visually scan the cube for patterns. Upon recognizing the patterns, I have to execute the most optimal algorithm to reach another another pattern repeatedly until the whole cube is solved. A classification model can learn the patterns, predict the best algorithms, and clock my turns per second. Essentially a classification model could be doing what I'm doing in parallel and just compare my performance with itself, much like a chess engine that is computing alongside the chess player. A really top notch cuber will turn the cubes so fluidly and controllably to be able to scan patterns, predict, and execute without seeming to stop.

## Question 2.2
**The files credit_card_data.txt (without headers) and credit_card_data-headers.txt (with headers) contain a dataset with 654 data points, 6 continuous and 4 binary predictor variables.  It has anonymized credit card applications with a binary response variable (last column) indicating if the application was positive or negative. The dataset is the “Credit Approval Data Set” from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Credit+Approval) without the categorical variables and without data points that have missing values.**

### Question 2.2.1
**Using the support vector machine function ksvm contained in the R package kernlab, find a good classifier for this data. Show the equation of your classifier, and how well it classifies the data points in the full data set. (Don’t worry about test/validation data yet; we’ll cover that topic soon.)**

Using the linear kernel (vanilladot) to test various c values to find the "best" model. C controls the margin of error in classification. In the module 2 lectures Dr. Sokol explains how he increases the margins or threshold for classifying mushrooms. Despite wild mushrooms looking and smelling like the ones in the grocery store, all mushrooms are assumed to be inedible. The high threshold ensures no one is hurt from consuming the wrong mushroom. The bigger the C the less risk of misclassifications. The smaller the C the more chance of misclassification. 

After the brute force method the most accurate rate is 86.39%. It's interesting that the accuracy is highest when the margins are lower and the accuracy decreases as the margin raises. This observation seems to go against the theory. Based on working with data scientists industry, I know that accuracy isn't the only metric for determining good or bad model. There are other metrics such as confusion matrix, F1 score, precision, etc to consider as well. 
```{r credit linear}
#------------------------- getting a sense of the data
# Read in credit_card_data data (source: https://teacherscollege.screenstepslive.com/a/1126998-import-data-into-r-txt-files-in-r-studio)
credit <- read.table("C://Users//Clair//OneDrive//Documents//Fall 2024//IYSE 6501//hw1//data 2.2//credit_card_data.txt", sep="\t", header=FALSE)
# View a summary of the dataset including basic statistics for each column
summary(credit)

# Get dimension of df, row x column
print("Size of df: ")
dim(credit) 

# Look for null values
print("Count of total missing values: ")
sum(is.na(credit))

#------------------------- manipulating the data
# convert .txt > df > matrix (source: https://stackoverflow.com/questions/46518838/how-to-convert-table-to-matrix-in-r)
credit_matrix <- data.matrix(credit)
head(credit_matrix)

#------------------------- helper
#install.packages("kernlab")
library(kernlab)

#------------------------- training the data
# Call ksvm(), Vanilladot is a simple linear kernel
# Train the model using the first 10 columns as features, 11th column is the target
# Parameter is c=x
model <- ksvm(credit_matrix[,1:10],as.factor(credit_matrix[,11]),type="C-svc",kernel="vanilladot",C=1,scaled=TRUE)                                            

# calculate a1…am
a <- colSums(model@xmatrix[[1]] * model@coef[[1]])
print("Equation: ")
print(a)

# calculate a0
a0 <- -model@b
print("a0 intercept: ")
print(a0)

#------------------------- predicting the data
# See what the model predicts
pred <- predict(model,credit_matrix[,1:10])
print("Prediction: ")
print(pred)

# See what fraction of the model’s predictions match the actual classification
accuracy <- sum(pred == credit_matrix[,11]) / nrow(credit_matrix)
accuracy
```

### 2.2.2
**You are welcome, but not required, to try other (nonlinear) kernels as well; we’re not covering them in this course, but they can sometimes be useful and might provide better predictions than vanilladot.**

Looking at the [R documentation page](https://search.r-project.org/CRAN/refmans/kernlab/html/ksvm.html), polynomial ```polydot``` and Radial basis function ```rbfdot``` are nonlinear kernels. The polynomial model seems to be the most consistent in high accuracy scores with sub 1% difference. Radial basis function model seems to behave as expected, the bigger the C the less risk of misclassifications. The smaller the C the more chance of misclassification. The accuracy metric seems more appropriate as evaluation for the nonlinear classification models compared to the linear classification model.

```{r credit nonlinear-polydot}
#------------------------- getting a sense of the data
# Read in credit_card_data data (source: https://teacherscollege.screenstepslive.com/a/1126998-import-data-into-r-txt-files-in-r-studio)
credit <- read.table("C://Users//Clair//OneDrive//Documents//Fall 2024//IYSE 6501//hw1//data 2.2//credit_card_data.txt", sep="\t", header=FALSE)

# View a summary of the dataset including basic statistics for each column
# summary(credit) didn't change my dataset so don't need to reprint summary

# Look for null values
#print("Count of total missing values: ")
#sum(is.na(credit)) didn't change my dataset so the missing values count have not changed

#------------------------- manipulating the data
# Convert .txt > df > matrix (source: https://stackoverflow.com/questions/46518838/how-to-convert-table-to-matrix-in-r)
credit_matrix <- data.matrix(credit)
#head(credit_matrix) didn't change my dataset so don't need to reprint summary

#------------------------- helper
#install.packages("kernlab")
library(kernlab)

#------------------------- loop through 10 models with varying margins
# Define a vector to store accuracy for each model
accuracies <- c()

# Loop through values of C from 100 to 1000 in steps of 100
for (C_value in seq(100, 1000, by=100)) {
  
  # Train an SVM model using the polydot kernel with varying C
  model <- ksvm(credit_matrix[,1:10], as.factor(credit_matrix[,11]), 
                type="C-svc", kernel="polydot", 
                kpar=list(degree=3), C=C_value, scaled=TRUE)
  
  # Predict the data using the trained model
  pred <- predict(model, credit_matrix[,1:10])
  
  # Calculate accuracy of the model by comparing predictions to actual class labels
  accuracy <- sum(pred == credit_matrix[,11]) / nrow(credit_matrix)
  
  # Print the C value and corresponding accuracy
  print(paste("C =", C_value, "-> Accuracy:", accuracy))
  
  # Store the accuracy for each model
  accuracies <- c(accuracies, accuracy)
}
```
```{r credit nonlinear-rbfdot}
#------------------------- getting a sense of the data
# Read in credit_card_data data (source: https://teacherscollege.screenstepslive.com/a/1126998-import-data-into-r-txt-files-in-r-studio)
credit <- read.table("C://Users//Clair//OneDrive//Documents//Fall 2024//IYSE 6501//hw1//data 2.2//credit_card_data.txt", sep="\t", header=FALSE)

# View a summary of the dataset including basic statistics for each column
# summary(credit) didn't change my dataset so don't need to reprint summary

# Look for null values
#print("Count of total missing values: ")
#sum(is.na(credit)) didn't change my dataset so the missing values count have not changed

#------------------------- manipulating the data
# Convert .txt > df > matrix (source: https://stackoverflow.com/questions/46518838/how-to-convert-table-to-matrix-in-r)
credit_matrix <- data.matrix(credit)
#head(credit_matrix) didn't change my dataset so don't need to reprint summary

#------------------------- helper
#install.packages("kernlab")
library(kernlab)

#------------------------- loop through 10 models with varying margins
# Define a vector to store accuracy for each model
accuracies <- c()

# Loop through values of C from 100 to 1000 by 100
for (C_value in seq(100, 1000, by=100)) {
  
  # Train an SVM model using the RBF kernel with varying C
  model <- ksvm(credit_matrix[,1:10], as.factor(credit_matrix[,11]), 
                type="C-svc", kernel="rbfdot", 
                C=C_value, scaled=TRUE)
  
  # Predict the data using the trained model
  pred <- predict(model, credit_matrix[,1:10])
  
  # Calculate accuracy of the model by comparing predictions to actual class labels
  accuracy <- sum(pred == credit_matrix[,11]) / nrow(credit_matrix)
  
  # Print the C value and corresponding accuracy
  print(paste("C =", C_value, "-> Accuracy:", accuracy))
  
  # Store the accuracy for each model
  accuracies <- c(accuracies, accuracy)
}
```

### Question 2.2.3
**Using the k-nearest-neighbors classification function kknn contained in the R kknn package, suggest a good value of k, and show how well it classifies that data points in the full data set. Don’t forget to scale the data (scale=TRUE in kknn).**

In the scaled KNN model, the highest accuracy is 81.50% with k=1, which indicates a high bias as it prefers itself. In contrast, the unscaled model achieves 85.32% accuracy with k=12. This suggests that the unscaled model, with a k value greater than 1, provides a more balanced representation of the data and better captures the realistic/organic variability in the data.

scaled
```{r credit knn scaled}
#------------------------- getting a sense of the data
# Reading in the credit card data
credit <- read.table("C://Users//Clair//OneDrive//Documents//Fall 2024//IYSE 6501//hw1//data 2.2//credit_card_data.txt", sep="\t", header=FALSE)

#------------------------- manipulating the data
# Convert to data frame and scale the features
credit_df <- as.data.frame(credit)
scaled_credit_df <- as.data.frame(scale(credit_df[, 1:10])) # scale df
scaled_credit_df$V11 <- as.factor(credit_df$V11)

#------------------------- helper libraries
#install.packages("kknn") (source:https://www.rdocumentation.org/packages/kknn/versions/1.3.1)
library(kknn)
library(ggplot2)

#------------------------- leave-one-out cross-validation using kknn with internal scaling
accuracies <- c()  # Vector to store accuracy for different K values

# Loop through values of K from 1 to 50
for (K_value in 1:50) {
  correct_predictions <- 0  # Count correct predictions
  
  # Loop through each row for leave-one-out cross-validation
  for (i in 1:nrow(credit_df)) {
    
    # Exclude the i-th data point for training
    train_data <- credit_df[-i, ]  # All but the i-th row for training
    test_data <- credit_df[i, , drop = FALSE]  # Only the i-th row for testing
    
    # Fit the kknn model with internal scaling
    model <- kknn(V11 ~ ., train = train_data, test = test_data, k = K_value, scale = TRUE)
    
    # Get the predicted label
    pred <- fitted(model)
    
    # Check if the prediction matches the true label
    if (as.character(pred) == as.character(test_data$V11)) {
      correct_predictions <- correct_predictions + 1
    }
  }
  
  # Calculate accuracy for this K value
  accuracy <- correct_predictions / nrow(credit_df)
  
  # Print the K value and corresponding accuracy
  print(paste("K =", K_value, "-> Accuracy:", accuracy))
  
  # Store the accuracy for each model
  accuracies <- c(accuracies, accuracy)
}

# Find the best K for the scaled model
best_k_scaled <- which.max(accuracies)
best_accuracy_scaled <- max(accuracies)

print(paste("Best K for scaled data:", best_k_scaled))
print(paste("Best accuracy for scaled data:", best_accuracy_scaled))

# Plotting the accuracy vs K value
ggplot(data.frame(K = 1:50, Accuracy = accuracies), aes(x = K, y = Accuracy)) +
  geom_line(color = "blue", size = 1) +    
  geom_point(color = "red", size = 3) +    
  labs(title = "KNN Accuracy for Different K Values (LOO-CV, scaled)",
       x = "K Value",
       y = "Accuracy") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)

```

unscaled
```{r credit knn unscaled}
#------------------------- getting a sense of the data
# Reading in the credit card data
credit <- read.table("C://Users//Clair//OneDrive//Documents//Fall 2024//IYSE 6501//hw1//data 2.2//credit_card_data.txt", sep="\t", header=FALSE)

#------------------------- manipulating the data
# Convert to data frame without scaling the features
credit_df <- as.data.frame(credit)
credit_df$V11 <- as.factor(credit_df$V11)  # Ensure the labels are treated as factors

#------------------------- helper libraries
# install.packages("kknn")
library(kknn)
library(ggplot2)

#------------------------- leave-one-out cross-validation using unscaled data and kknn
accuracies <- c()  # Vector to store accuracy for different K values

# Loop through values of K from 1 to 50
for (K_value in 1:50) {
  correct_predictions <- 0  # Count correct predictions
  
  # Loop through each row for leave-one-out cross-validation
  for (i in 1:nrow(credit_df)) {
    
    # Exclude the i-th data point for training
    train_data <- credit_df[-i, ]  # All but the i-th row for training
    test_data <- credit_df[i, , drop = FALSE]  # Only the i-th row for testing
    
    # Fit the kknn model for this split using unscaled data
    model <- kknn(V11 ~ ., train = train_data, test = test_data, k = K_value)
    
    # Get the predicted label
    pred <- fitted(model)
    
    # Check if the prediction matches the true label
    if (as.character(pred) == as.character(test_data$V11)) {
      correct_predictions <- correct_predictions + 1
    }
  }
  
  # Calculate accuracy for this K value
  accuracy <- correct_predictions / nrow(credit_df)
  
  # Print the K value and corresponding accuracy
  print(paste("K =", K_value, "-> Accuracy:", accuracy))
  
  # Store the accuracy for each model
  accuracies <- c(accuracies, accuracy)
}

#------------------------- output final accuracies for each model
print("Accuracies for each KNN model with varying K:")
print(accuracies)
print("Max accuracy for leave-one-out cross-validation:")
print(max(accuracies))

#------------------------- plotting the results using ggplot2
# Create a dataframe with K values and corresponding accuracies
accuracy_data <- data.frame(
  K = 1:50,
  Accuracy = accuracies
)

# Plotting the accuracy vs K value
ggplot(accuracy_data, aes(x = K, y = Accuracy)) +
  geom_line(color = "blue", size = 1) +    
  geom_point(color = "red", size = 3) +    
  labs(title = "KNN Accuracy for Different K Values (LOO-CV, Unscaled)",
       x = "K Value",
       y = "Accuracy") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent)

# Find the best K for the unscaled model
best_k_unscaled <- which.max(accuracies)
best_accuracy_unscaled <- max(accuracies)

print(paste("Best K for unscaled data:", best_k_unscaled))
print(paste("Best accuracy for unscaled data:", best_accuracy_unscaled))
```